
@online{deepl_deepl_nodate,
	title = {{DeepL} Übersetzer: Der präziseste Übersetzer der Welt},
	url = {https://www.deepl.com/translator},
	shorttitle = {{DeepL} Übersetzer},
	abstract = {Übersetzen Sie Texte und ganze Dateien im Handumdrehen. Präzise Übersetzungen für Einzelnutzer und Teams. Jeden Tag nutzen Millionen von Menschen {DeepL}.},
	author = {{DeepL}},
	urldate = {2024-01-20},
	langid = {german},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\WBC8DB23\\translator.html:text/html},
}

@online{openai_chatgpt_nodate,
	title = {{ChatGPT}},
	url = {https://chat.openai.com},
	abstract = {A conversational {AI} system that listens, learns, and challenges},
	author = {{OpenAI}},
	urldate = {2024-01-20},
	langid = {american},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\Q9LTMZPS\\chat.openai.com.html:text/html},
}

@book{raschka_build_2025,
	title = {Build a Large Language Model (From Scratch)},
	isbn = {978-1-63343-716-6},
	url = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
	abstract = {Learn how to create, train, and tweak large language models ({LLMs}) by building one from the ground up!{\textless}/b{\textgreater}

In Build a Large Language Model (from Scratch){\textless}/i{\textgreater}, you’ll discover how {LLMs} work from the inside out. In this insightful book, bestselling author Sebastian Raschka guides you step by step through creating your own {LLM}, explaining each stage with clear text, diagrams, and examples.  You’ll go from the initial design and creation to pretraining on a general corpus, all the way to finetuning for specific tasks.

Build a Large Language Model (from Scratch){\textless}/i{\textgreater} teaches you how to:

Plan and code all the parts of an {LLM}{\textless}/li{\textgreater}
Prepare a dataset suitable for {LLM} training{\textless}/li{\textgreater}
Finetune {LLMs} for text classification and with your own data{\textless}/li{\textgreater}
Use human feedback to ensure your {LLM} follows instructions{\textless}/li{\textgreater}
Load pretrained weights into an {LLM}{\textless}/li{\textgreater}
{\textless}/ul{\textgreater}

The large language models ({LLMs}) that power cutting-edge {AI} tools like {ChatGPT}, Bard, and Copilot seem like a miracle, but they’re not magic. This book demystifies {LLMs} by helping you build your own from scratch. You’ll get a unique and valuable insight into how {LLMs} work, learn how to evaluate their quality, and pick up concrete techniques to finetune and improve them.

The process you use to train and develop your own small-but-functional model in this book follows the same steps used to deliver huge-scale foundation models like {GPT}-4. Your small-scale {LLM} can be developed on an ordinary laptop, and you’ll be able to use it as your own personal assistant.},
	publisher = {Manning},
	author = {Raschka, Sebastian},
	urldate = {2024-01-20},
	date = {2025},
	langid = {english},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\VKGU3IIF\\build-a-large-language-model-from-scratch.html:text/html},
}

@article{radford_improving_nodate,
	title = {Improving Language Understanding by Generative Pre-Training},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering ({RACE}), and 1.5\% on textual entailment ({MultiNLI}).},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	langid = {english},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:C\:\\Users\\sebas\\Zotero\\storage\\BC64HL96\\Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2024-01-20},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\sebas\\Zotero\\storage\\7H8RBQGI\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\8VI2PQDS\\1810.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-01-20},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\sebas\\Zotero\\storage\\QXSBYBAY\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\V4ZZKXN7\\1706.html:text/html},
}

@book{helmold_kaizen_2021,
	location = {Wiesbaden},
	title = {Kaizen, Lean Management und Digitalisierung: Mit den japanischen Konzepten Wettbewerbsvorteile für das Unternehmen erzielen},
	isbn = {978-3-658-32341-7 978-3-658-32342-4},
	url = {https://link.springer.com/10.1007/978-3-658-32342-4},
	shorttitle = {Kaizen, Lean Management und Digitalisierung},
	publisher = {Springer Fachmedien},
	author = {Helmold, Marc},
	urldate = {2024-01-20},
	date = {2021},
	langid = {german},
	doi = {10.1007/978-3-658-32342-4},
	keywords = {Culture und Leadership Management, Führung, Führungsphilosophie, Japanische Erfolgskonzepte Unternehmensalltag, Japanische Philosophie, Kaizen, Lean Management, Nachhaltigkeit, Toyota Produktionssystem, Transformation schlankes Unternehmen, Unternehmenskultur verändern, Wertschöpfung und Verschwendung},
	file = {Eingereichte Version:C\:\\Users\\sebas\\Zotero\\storage\\MDU54EEH\\Helmold - 2021 - Kaizen, Lean Management und Digitalisierung Mit d.pdf:application/pdf},
}

@book{goldratt_goal_2012,
	location = {Great Barrington, Mass},
	edition = {3rd Revised, 30th Anniversary ed},
	title = {The Goal: A Process of Ongoing Improvement - 30th Anniversary Edition},
	isbn = {978-0-88427-195-6},
	shorttitle = {The Goal},
	abstract = {Written in a fast-paced thriller style, 'The Goal' contains a serious message for all managers in industry and explains the ideas which underline the Theory of Constraints developed by the author.},
	pagetotal = {408},
	publisher = {North River Press},
	author = {Goldratt, Eliyahu M. and Cox, Jeff and Whitford, David},
	date = {2012-06-01},
}

@book{goldratt_its_2011,
	edition = {1},
	title = {It's Not Luck},
	abstract = {A Business Novel by Eliyahu M.Goldratt. Learn how to apply {TOC} to sales, marketing, inventory control and production distribution.},
	pagetotal = {290},
	publisher = {North River Press},
	author = {Goldratt, Eliyahu M.},
	date = {2011-07-12},
}

@book{goldratt_critical_2002,
	location = {Great Barrington, Mass},
	edition = {First Edition},
	title = {Critical Chain},
	isbn = {978-0-88427-153-6},
	abstract = {A business novel focusing on project management. The novel aims to provoke readers to examine and reassess their business practices and transform the thinking and actions of managers.},
	pagetotal = {246},
	publisher = {The North River Press},
	author = {Goldratt, Eliyahu M.},
	date = {2002-12-10},
}

@book{bungay_art_2011,
	edition = {Illustrated Edition},
	title = {The Art of Action: How Leaders Close the Gaps between Plans, Actions and Results},
	shorttitle = {The Art of Action},
	abstract = {What do you want me to do? This question is the enduring management issue, a perennial problem that Stephen Bungay shows has an old solution that is counter-intuitive and yet common sense. The Art of Action is a thought-provoking and fresh look at how managers can turn planning into execution, and execution into results. Drawing on his experience as a consultant, senior manager and a highly respected military historian, Stephen Bungay takes a close look at the nineteenth-century Prussian Army, which built its agility on the initiative of its highly empowered junior officers, to show business leaders how they can build more effective, productive organizations. Based on a theoretical framework which has been tested in practice over 150 years, Bungay shows how the approach known as 'mission command' has been applied in businesses as diverse as pharmaceuticals and F1 racing today. The Art of Action is scholarly but engaging, rigorous but pragmatic, and shows how common sense can sometimes be surprising.},
	pagetotal = {306},
	publisher = {Nicholas Brealey Publishing},
	author = {Bungay, Stephen},
	date = {2011-02-16},
}

@book{imai1986kaizen,
	title     = {Kaizen: The Key to Japan's Competitive Success},
	author    = {Masaaki Imai},
	year      = {1986},
	publisher = {McGraw-Hill/Irwin}
}

@misc{ibscmr2003toyotakaizen,
	author       = {IBS Center for Management Research},
	title        = {Toyota's Kaizen Experience},
	year         = {2003},
	howpublished = {ICMR (IBS Center for Management Research)},
	url          = {https://www.icmrindia.org/casestudies/catalogue/Operations/Toyota%20Kaizen%20Experience-Operations%20Case%20Study.htm}
}


@misc{casecentre2002toyotakaizen,
	author       = {A Mukund and K Subhadra},
	title        = {Toyota's Kaizen Experience},
	year         = {2002},
	howpublished = {The Case Centre},
	url          = {https://www.thecasecentre.org/products/view?id=21675}
}
