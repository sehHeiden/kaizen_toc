
@online{deepl_deepl_nodate,
	title = {{DeepL} Übersetzer: Der präziseste Übersetzer der Welt},
	url = {https://www.deepl.com/translator},
	shorttitle = {{DeepL} Übersetzer},
	abstract = {Übersetzen Sie Texte und ganze Dateien im Handumdrehen. Präzise Übersetzungen für Einzelnutzer und Teams. Jeden Tag nutzen Millionen von Menschen {DeepL}.},
	author = {{DeepL}},
	urldate = {2024-01-20},
	langid = {german},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\WBC8DB23\\translator.html:text/html},
}

@online{openai_chatgpt_nodate,
	title = {{ChatGPT}},
	url = {https://chat.openai.com},
	abstract = {A conversational {AI} system that listens, learns, and challenges},
	author = {{OpenAI}},
	urldate = {2024-01-20},
	langid = {american},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\Q9LTMZPS\\chat.openai.com.html:text/html},
}

@book{raschka_build_2025,
	title = {Build a Large Language Model (From Scratch)},
	isbn = {978-1-63343-716-6},
	url = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
	abstract = {Learn how to create, train, and tweak large language models ({LLMs}) by building one from the ground up!{\textless}/b{\textgreater}

In Build a Large Language Model (from Scratch){\textless}/i{\textgreater}, you’ll discover how {LLMs} work from the inside out. In this insightful book, bestselling author Sebastian Raschka guides you step by step through creating your own {LLM}, explaining each stage with clear text, diagrams, and examples.  You’ll go from the initial design and creation to pretraining on a general corpus, all the way to finetuning for specific tasks.

Build a Large Language Model (from Scratch){\textless}/i{\textgreater} teaches you how to:

Plan and code all the parts of an {LLM}{\textless}/li{\textgreater}
Prepare a dataset suitable for {LLM} training{\textless}/li{\textgreater}
Finetune {LLMs} for text classification and with your own data{\textless}/li{\textgreater}
Use human feedback to ensure your {LLM} follows instructions{\textless}/li{\textgreater}
Load pretrained weights into an {LLM}{\textless}/li{\textgreater}
{\textless}/ul{\textgreater}

The large language models ({LLMs}) that power cutting-edge {AI} tools like {ChatGPT}, Bard, and Copilot seem like a miracle, but they’re not magic. This book demystifies {LLMs} by helping you build your own from scratch. You’ll get a unique and valuable insight into how {LLMs} work, learn how to evaluate their quality, and pick up concrete techniques to finetune and improve them.

The process you use to train and develop your own small-but-functional model in this book follows the same steps used to deliver huge-scale foundation models like {GPT}-4. Your small-scale {LLM} can be developed on an ordinary laptop, and you’ll be able to use it as your own personal assistant.},
	publisher = {Manning},
	author = {Raschka, Sebastian},
	urldate = {2024-01-20},
	date = {2025},
	langid = {english},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\VKGU3IIF\\build-a-large-language-model-from-scratch.html:text/html},
}

@article{radford_improving_nodate,
	title = {Improving Language Understanding by Generative Pre-Training},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering ({RACE}), and 1.5\% on textual entailment ({MultiNLI}).},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	langid = {english},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:C\:\\Users\\sebas\\Zotero\\storage\\BC64HL96\\Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2024-01-20},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\sebas\\Zotero\\storage\\7H8RBQGI\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\8VI2PQDS\\1810.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-01-20},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\sebas\\Zotero\\storage\\QXSBYBAY\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\V4ZZKXN7\\1706.html:text/html},
}

@book{helmold_kaizen_2021,
	location = {Wiesbaden},
	title = {Kaizen, Lean Management und Digitalisierung: Mit den japanischen Konzepten Wettbewerbsvorteile für das Unternehmen erzielen},
	isbn = {978-3-658-32341-7 978-3-658-32342-4},
	url = {https://link.springer.com/10.1007/978-3-658-32342-4},
	shorttitle = {Kaizen, Lean Management und Digitalisierung},
	publisher = {Springer Fachmedien},
	author = {Helmold, Marc},
	urldate = {2024-01-20},
	date = {2021},
	langid = {german},
	doi = {10.1007/978-3-658-32342-4},
	keywords = {Culture und Leadership Management, Führung, Führungsphilosophie, Japanische Erfolgskonzepte Unternehmensalltag, Japanische Philosophie, Kaizen, Lean Management, Nachhaltigkeit, Toyota Produktionssystem, Transformation schlankes Unternehmen, Unternehmenskultur verändern, Wertschöpfung und Verschwendung},
	file = {Eingereichte Version:C\:\\Users\\sebas\\Zotero\\storage\\MDU54EEH\\Helmold - 2021 - Kaizen, Lean Management und Digitalisierung Mit d.pdf:application/pdf},
}

@book{goldratt_goal_2012,
	location = {Great Barrington, Mass},
	edition = {3rd Revised, 30th Anniversary ed},
	title = {The Goal: A Process of Ongoing Improvement - 30th Anniversary Edition},
	isbn = {978-0-88427-195-6},
	shorttitle = {The Goal},
	abstract = {Written in a fast-paced thriller style, 'The Goal' contains a serious message for all managers in industry and explains the ideas which underline the Theory of Constraints developed by the author.},
	pagetotal = {408},
	publisher = {North River Press},
	author = {Goldratt, Eliyahu M. and Cox, Jeff and Whitford, David},
	date = {2012-06-01},
}

@book{goldratt_its_2011,
	edition = {1},
	title = {It's Not Luck},
	abstract = {A Business Novel by Eliyahu M.Goldratt. Learn how to apply {TOC} to sales, marketing, inventory control and production distribution.},
	pagetotal = {290},
	publisher = {North River Press},
	author = {Goldratt, Eliyahu M.},
	date = {2011-07-12},
}

@book{goldratt_critical_2002,
	location = {Great Barrington, Mass},
	edition = {First Edition},
	title = {Critical Chain},
	isbn = {978-0-88427-153-6},
	abstract = {A business novel focusing on project management. The novel aims to provoke readers to examine and reassess their business practices and transform the thinking and actions of managers.},
	pagetotal = {246},
	publisher = {The North River Press},
	author = {Goldratt, Eliyahu M.},
	date = {2002-12-10},
}

@book{bungay_art_2011,
	edition = {Illustrated Edition},
	title = {The Art of Action: How Leaders Close the Gaps between Plans, Actions and Results},
	shorttitle = {The Art of Action},
	abstract = {What do you want me to do? This question is the enduring management issue, a perennial problem that Stephen Bungay shows has an old solution that is counter-intuitive and yet common sense. The Art of Action is a thought-provoking and fresh look at how managers can turn planning into execution, and execution into results. Drawing on his experience as a consultant, senior manager and a highly respected military historian, Stephen Bungay takes a close look at the nineteenth-century Prussian Army, which built its agility on the initiative of its highly empowered junior officers, to show business leaders how they can build more effective, productive organizations. Based on a theoretical framework which has been tested in practice over 150 years, Bungay shows how the approach known as 'mission command' has been applied in businesses as diverse as pharmaceuticals and F1 racing today. The Art of Action is scholarly but engaging, rigorous but pragmatic, and shows how common sense can sometimes be surprising.},
	pagetotal = {306},
	publisher = {Nicholas Brealey Publishing},
	author = {Bungay, Stephen},
	date = {2011-02-16},
}

@book{imai1986kaizen,
	title     = {Kaizen: The Key to Japan's Competitive Success},
	author    = {Masaaki Imai},
	year      = {1986},
	publisher = {McGraw-Hill/Irwin}
}

@misc{ibscmr2003toyotakaizen,
	author       = {IBS Center for Management Research},
	title        = {Toyota's Kaizen Experience},
	year         = {2003},
	howpublished = {ICMR (IBS Center for Management Research)},
	url          = {https://www.icmrindia.org/casestudies/catalogue/Operations/Toyota%20Kaizen%20Experience-Operations%20Case%20Study.htm}
}


@misc{casecentre2002toyotakaizen,
	author       = {A Mukund and K Subhadra},
	title        = {Toyota's Kaizen Experience},
	year         = {2002},
	howpublished = {The Case Centre},
	url          = {https://www.thecasecentre.org/products/view?id=21675}
}

@online{noauthor_sprachen_nodate,
	title = {Sprachen im Internet nach Anteil der Websites 2023},
	url = {https://de.statista.com/statistik/daten/studie/2961/umfrage/anteil-der-verbreitetsten-sprachen-im-internet/},
	abstract = {Wie viele Webseiten sind auf Englisch? Über die Hälfte der Webseiten sind auf Englisch. Mit großem Abstand folgt der Anteil der Websites auf Spanisch.},
	titleaddon = {Statista},
	urldate = {2024-01-28},
	langid = {german},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\3JY3R9HQ\\anteil-der-verbreitetsten-sprachen-im-internet.html:text/html},
}


@online{noauthor_heat_nodate,
	title = {The Heat Is On: How The Kaizen Approach Is Helping {GE} Gas Power With Carbon Emissions And Energy Costs {\textbar} {GE} News},
	url = {https://www.ge.com/news/reports/the-heat-is-on-how-the-kaizen-approach-is-helping-ge-gas-power-with-carbon-emissions-and},
	shorttitle = {The Heat Is On},
	abstract = {The Heat Is On: How The Kaizen Approach Is Helping {GE} Gas Power With Carbon Emissions And Energy Costs},
	urldate = {2024-01-28},
	langid = {english},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\IUBP8WGQ\\the-heat-is-on-how-the-kaizen-approach-is-helping-ge-gas-power-with-carbon-emissions-and.html:text/html},
}


@online{noauthor_what_nodate,
	title = {What is {TOC} - Theory of Constraints International Certification Organization},
	url = {https://www.tocico.org/page/WhatisTOCoverview},
	urldate = {2024-01-28},
	file = {What is TOC - Theory of Constraints International Certification Organization:C\:\\Users\\sebas\\Zotero\\storage\\PVSJD94M\\WhatisTOCoverview.html:text/html},
}


@online{noauthor_kaizen_nodate,
	title = {kaizen Beispiele},
	url = {https://www.2ease.org/kaizen-besserwerden/kaizen-beispiele},
	abstract = {Kaizen Beispiele die beeindrucken. Porsche wird vor Übernahme bewahrt und Saia in der Schweiz kommt mit 40\% Aufwertung des Frankens zurecht. Sehen Sie wie!},
	titleaddon = {Doing-better Führungsinstrumente \& Kaizen},
	urldate = {2024-01-28},
	langid = {german},
	file = {Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\LU4LG2D9\\kaizen-beispiele.html:text/html},
}


@article{noy_experimental_2023,
	title = {Experimental evidence on the productivity effects of generative artificial intelligence},
	volume = {381},
	url = {https://www.science.org/doi/10.1126/science.adh2586},
	doi = {10.1126/science.adh2586},
	abstract = {We examined the productivity effects of a generative artificial intelligence ({AI}) technology, the assistive chatbot {ChatGPT}, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to {ChatGPT}. Our results show that {ChatGPT} substantially raised productivity: The average time taken decreased by 40\% and output quality rose by 18\%. Inequality between workers decreased, and concern and excitement about {AI} temporarily rose. Workers exposed to {ChatGPT} during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.},
	pages = {187--192},
	number = {6654},
	journaltitle = {Science},
	author = {Noy, Shakked and Zhang, Whitney},
	urldate = {2024-01-28},
	date = {2023-07-14},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Noy und Zhang - Experimental Evidence on the Productivity Effects .pdf:C\:\\Users\\sebas\\Zotero\\storage\\MRHHVINS\\Noy und Zhang - Experimental Evidence on the Productivity Effects .pdf:application/pdf},
}

@book{womack1996lean,
	title={Lean Thinking: Banish Waste and Create Wealth in Your Corporation},
	author={Womack, James P. and Jones, Daniel T.},
	year={1996},
	publisher={Simon and Schuster}
	
@article{gundogar_dynamic_2016,
title = {Dynamic bottleneck elimination in mattress manufacturing line using theory of constraints},
volume = {5},
issn = {2193-1801},
url = {https://doi.org/10.1186/s40064-016-2947-1},
doi = {10.1186/s40064-016-2947-1},
abstract = {There is a tough competition in the furniture sector like other sectors. Along with the varying product range, production system should also be renewed on a regular basis and the production costs should be kept under control. In this study, spring mattress manufacturing line of a furniture manufacturing company is analyzed. The company wants to increase its production output with new investments. The objective is to find the bottlenecks in production line in order to balance the semi-finished material flow. These bottlenecks are investigated and several different scenarios are tested to improve the current manufacturing system. The problem with a main theme based on the elimination of the bottleneck is solved using Goldratt and Cox’s theory of constraints with a simulation based heuristic method. Near optimal alternatives are determined by system models built in Arena 13.5 simulation software. Results show that approximately 46 \% capacity enhancements with 2 buffer stocks have increased average production by 88.8 \%.},
pages = {1276},
number = {1},
journaltitle = {{SpringerPlus}},
shortjournal = {{SpringerPlus}},
author = {Gundogar, Emin and Sari, Murat and Kokcam, Abdullah H.},
urldate = {2024-01-28},
date = {2016-08-08},
keywords = {Bottleneck search, Buffer stock, Simulation, Spring mattress manufacturing, Theory of constraints},
file = {Full Text PDF:C\:\\Users\\sebas\\Zotero\\storage\\DVQF2CWN\\Gundogar et al. - 2016 - Dynamic bottleneck elimination in mattress manufac.pdf:application/pdf;Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\8LUCQW3Z\\s40064-016-2947-1.html:text/html},
}


@article{thurer_bottleneck-oriented_2018,
title = {Bottleneck-oriented order release with shifting bottlenecks: An assessment by simulation},
volume = {197},
issn = {0925-5273},
url = {https://www.sciencedirect.com/science/article/pii/S0925527318300409},
doi = {10.1016/j.ijpe.2018.01.010},
shorttitle = {Bottleneck-oriented order release with shifting bottlenecks},
abstract = {Bottleneck shiftiness is an important managerial problem that has received significant research attention. The extant literature has shown, for example, that protective capacity reduces the likelihood of the bottleneck shifting. Yet the actual performance impact of a bottleneck shift has been widely neglected. We posit that there are at least two interrelated effects that may impact shop performance: (i) the direct effect of the change in bottleneck position; and, (ii) the indirect effect of the order release method incorrectly identifying the bottleneck (i.e. assuming the bottleneck is Station X when it is actually Station Y). The latter is particularly acute in the context of bottleneck-oriented order release methods such as Drum-Buffer-Rope ({DBR}) as these release methods use feedback from the (assumed) bottleneck to control release. Using controlled simulation experiments we demonstrate that a bottleneck shift to a station upstream of the assumed bottleneck has a negligible effect on {DBR} performance while a downstream shift is detrimental to performance. Meanwhile, the distance, i.e. the number of stations between the actual and assumed bottleneck, has a negligible performance impact. These results have important managerial and research implications for {DBR} and other release methods.},
pages = {275--282},
journaltitle = {International Journal of Production Economics},
shortjournal = {International Journal of Production Economics},
author = {Thürer, Matthias and Stevenson, Mark},
urldate = {2024-01-28},
date = {2018-03-01},
keywords = {Bottleneck position, Bottleneck shiftiness, Constant work-in-process, Drum-Buffer-Rope, Simulation, Workload control},
file = {Akzeptierte Version:C\:\\Users\\sebas\\Zotero\\storage\\IW2KD4MH\\Thürer und Stevenson - 2018 - Bottleneck-oriented order release with shifting bo.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\sebas\\Zotero\\storage\\IHNK8F8M\\S0925527318300409.html:text/html},
}


@article{umble_implementing_2006,
title = {Implementing theory of constraints in a traditional Japanese manufacturing environment: The case of Hitachi Tool Engineering},
volume = {44},
issn = {0020-7543},
url = {https://doi.org/10.1080/00207540500381393},
doi = {10.1080/00207540500381393},
shorttitle = {Implementing theory of constraints in a traditional Japanese manufacturing environment},
abstract = {This research presents a case study of a virtual ‘textbook’ application of the theory of constraints ({TOC}) in a Japanese tool manufacturing company. Hitachi Tool Engineering uses state-of-the-art technology to design and manufacture cutting tools known as End-mills. The plant described in this study is a classic V-plant and exhibited all of the standard problems of a traditionally managed V-plant, existing within the unique framework of Japanese work culture. Plant management applied the five focusing steps and used the operations strategy tools, including drum-buffer-rope and buffer management, to improve the system. Following the approach recommended by Eli Goldratt, the thinking process tools of current reality tree and evaporating clouds were used to help identify and resolve problems when the implementation encountered major obstacles. While the implementation was a huge success, the devastating effect of a core problem being left unresolved is well documented. The implementation generated significant improvements in work-in-process inventory, production lead time, on-time delivery, productive capacity, inventory turnover, product quality, sales volume, and profitability. Moreover, management has extended the introduction of {TOC} to the non-manufacturing functions and {TOC} is becoming the common company culture that bridges four culturally diverse manufacturing plants.},
pages = {1863--1880},
number = {10},
journaltitle = {International Journal of Production Research},
author = {Umble, M. and Umble, E. and Murakami, S.},
urldate = {2024-01-28},
date = {2006-05-15},
note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00207540500381393},
keywords = {Japanese manufacturing, Theory of constraints, Thinking processes, Throughput accounting},
}


